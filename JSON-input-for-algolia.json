[{"authors":["abtris"],"categories":null,"content":"I’m a software engineer focused on site reliability and product development (SaaS, PaaS or IaaS). I like backend programming in Go, Ruby and Node.JS.\nI’m improving processes and workflows about oncall, monitoring, development and software architecture in cloud.\nI\u0026rsquo;m Prague Golang Meetup Organizer. Want to learn more about Golang? Check our website and join us on meetup.com. Recording are available on youtube.\nI\u0026rsquo;m Co-Host Podcast You Build It, You Run It.\n","date":1710069730,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1710069730,"objectID":"b1c3e8814fc93f46c558271214dca8af","permalink":"https://www.prskavec.net/authors/abtris/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/abtris/","section":"authors","summary":"I’m a software engineer focused on site reliability and product development (SaaS, PaaS or IaaS). I like backend programming in Go, Ruby and Node.JS.\nI’m improving processes and workflows about oncall, monitoring, development and software architecture in cloud.\nI\u0026rsquo;m Prague Golang Meetup Organizer. Want to learn more about Golang? Check our website and join us on meetup.com. Recording are available on youtube.\nI\u0026rsquo;m Co-Host Podcast You Build It, You Run It.","tags":null,"title":"Ladislav Prskavec","type":"authors"},{"authors":null,"categories":null,"content":"Guide to good On-call that does not kill your team You can read quotes as this across the whole cloud industry:\nSome teams have had dramatically different pager volumes for years at my company. There are teams which people like to work on thanks to their great mission, but those same teams have a noisy oncall. There is no way I would move to those teams, where moving would come with so much unpaid overtime at unsociable hours.” From a publicly traded tech company valued at $13B.\nMaking good on-call across various teams isn\u0026rsquo;t an easy job. It would be best if you had great support from senior executives. The team manager needs to know where to push for hiring and how to make priorities for the team to avoid toil that is not manageable.\nWe don\u0026rsquo;t cover details about incident response itself from engineers being on-call. Please read excellent documentation from Pagerduty to familiarize yourself with the more detailed guide on working during the incident1.\nHow introduce On-Call into your company? Introducing On-call is a major culture change for people. If you have never done it before, it significantly impacts your personal life, and you get a lot of stress that you don\u0026rsquo;t have before. The company should have asked people and have this option in contracts if you need it. It isn\u0026rsquo;t lovely if on-call is a surprise or you want to be promoted after your probation period.\nIf you are a small startup is good to start early during working hours. I will focus on good observability and alerting with proper paging workflow and get ownership to developers working on the service. This approach is called You build it; you run it. 2\nI will recommend taking your time with changes. You have to be sure that all people who will join on-call fully understand what is behind that extra work. You should give them proper training using gamedays 3. I recommend having a training environment; everyone should go through a few incidents to be sure of what to do. The mastered process is more important than the solution for good on-call. For solutions, you can always summon engineers across the company if you need them. Communicating with all stakeholders and controlling what happens is critical to good incident management.\nHow Many People Do You Need For On-call? \u0026ldquo;Using the 25% on-call rule, we can derive the minimum number of SREs required to sustain a 24/7 on-call rotation. Assuming that there are always two people on-call (primary and secondary, with different duties), the minimum number of engineers needed for on-call duty from a single-site team is eight: assuming week-long shifts, each engineer is on-call (primary or secondary) for one week every month. For dual-site teams, a reasonable minimum size of each team is six, both to honor the 25% rule and to ensure a substantial and critical mass of engineers for the team.\u0026rdquo; - SRE book 4\nAs explained in the quote above, Google recommends a minimum of 8 engineers for one site and six engineers per site in the case of two sites. My calculations show that you could have fewer engineers on-call if they are divided between two or three locations, but you have to always keep in mind vacations, sick days, business travels, etc.\nI remember making a full on-call rotation with six people working 24/7/365 — it was exhausting and significantly impacted people\u0026rsquo;s personal lives. As an SRE manager, I always try to focus on achieving good work \u0026amp; time off balance for people being a part of an on-call rotation. The burden of providing non-stop support can be a significant reason why people leave engineering teams.\nPagerDuty Incident Response process. It is a cut-down version of our internal documentation used at PagerDuty for any major incidents and to prepare new employees for on-call responsibilities - https://response.pagerduty.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n“You build it, you run it” is a software development principle that emphasizes the responsibility of the development team in designing, building, and maintaining the systems they create.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGamedays - So, what exactly is a GameDay? A GameDay is a period of time (usually 1—4 hours) set aside for a team to run one or more experiments on a system, process or service, observe the impact, then discuss the outcomes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBetsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy. 2016. Site Reliability Engineering: How Google Runs Production Systems (1st. ed.). O\u0026rsquo;Reilly Media, Inc. - https://landing.google.com/sre/sre-book/chapters/being-on-call/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1621382400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1621382400,"objectID":"adbb775c364048becc936bddab483bd7","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/","publishdate":"2021-05-19T00:00:00Z","relpermalink":"/courses/how-to-make-oncall/","section":"courses","summary":"Learn how to design OnCall for your team.","tags":null,"title":"How to design OnCall","type":"docs"},{"authors":null,"categories":null,"content":"The roster is a term used for on-call planning, which describes a group of people who are part of a single on-call rotation. The number of roster layers you have for escalations depends on your company’s standards, industry requirements, and the particular composition of your team. The most commonly used setup includes a primary and secondary on-call in which the role of the secondary is to step up if the primary is overwhelmed. Typically, engineers in both rosters have the similar skills.\nAn alternative is Tier 1 on-call which escalated to Tier 2. Tier 2 consists of incident commanders/veteran on-call engineers with elevated access rights capable of resolving even the most complex incidents.\nYou can have multiple teams, and every team can have a different roster. Still, they can share responsibility for a user support on-call that isn’t directly related to production systems.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"5f398005a3e8b28d2c4ff304ae2920c4","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter01/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter01/","section":"courses","summary":"The roster is a term used for on-call planning, which describes a group of people who are part of a single on-call rotation. The number of roster layers you have for escalations depends on your company’s standards, industry requirements, and the particular composition of your team. The most commonly used setup includes a primary and secondary on-call in which the role of the secondary is to step up if the primary is overwhelmed.","tags":null,"title":"Roster","type":"docs"},{"authors":null,"categories":null,"content":"One of the challenging problems of on-call design is controlling the number of times engineers are paged during the night. It is essential for single-site teams. Having a high number of pages can cause alarm fatigue. Having people answer pages at 2 AM only to realize they are responding to something that isn’t urgent or actionable can quickly damage the team’s morale.\nYou have to work on fine-tuning the alerting to ensure that anything that isn’t urgent or critical will wait until the on-call engineer wakes up. Sometimes this may require increasing alarm thresholds. In other cases, it may require code changes to improve error handling. While this may look like additional work, it can help ensure that on-call work is sustainable and protect your team from burnout.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"70e61fd360bf6d1e2354ad584130ac8c","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter02/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter02/","section":"courses","summary":"One of the challenging problems of on-call design is controlling the number of times engineers are paged during the night. It is essential for single-site teams. Having a high number of pages can cause alarm fatigue. Having people answer pages at 2 AM only to realize they are responding to something that isn’t urgent or actionable can quickly damage the team’s morale.\nYou have to work on fine-tuning the alerting to ensure that anything that isn’t urgent or critical will wait until the on-call engineer wakes up.","tags":null,"title":"High Alert ratio","type":"docs"},{"authors":null,"categories":null,"content":"You need to consider a few things if you have primary and secondary.\nWill rotate the same people on both rosters? Give more junior people to primary and more experience to secondary? If people rotate on both rosters, what happens if both people don’t know what to do? How do we make escalations if both levels fail? Do I need some manager on-call or not? Do I want to page all members of the team instead? ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"f9032e7b18cec403ea918b3d47ff9cd0","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter03/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter03/","section":"courses","summary":"You need to consider a few things if you have primary and secondary.\nWill rotate the same people on both rosters? Give more junior people to primary and more experience to secondary? If people rotate on both rosters, what happens if both people don’t know what to do? How do we make escalations if both levels fail? Do I need some manager on-call or not? Do I want to page all members of the team instead?","tags":null,"title":"Primary vs secondary","type":"docs"},{"authors":null,"categories":null,"content":"My example is about weekly shifts, which isn\u0026rsquo;t why I prefer weekly shifts. Weekly shifts are not better or worse. I think that if you are one site team, daily shifts are better. You can work weekends extra and make six people deal with the whole week. For 2-3 sites, weekly shifts are more common. A strategy named \u0026ldquo;follow the sun\u0026rdquo; (FTS 1) is usually defined by omitting the night shift from oncall that can be done in 2 or 3 sites. You have many handovers, and this is something that you have to prepare carefully.\nI think from my experience, daily on-call is better for work time life balance. You don\u0026rsquo;t have a whole week with a computer, and you can quickly move your program to the evening movie or theater if it is just one day. You switch shifts with a colleague more easily.\nYou can make some custom scheme that fits you, but simplicity is key here too. For more complex tiers, you need tooling that checks if the same person doesn\u0026rsquo;t have primary and secondary at the same time. That looks obvious, but I have seen it many times people that happen, and it\u0026rsquo;s not suitable for that person. My friend mentioned that this happened to a first shift new on-call team member on his first shift, and It was over the weekend, and others found it on Monday.\nhttps://en.wikipedia.org/wiki/Follow-the-sun\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"c7ad135a34b2810e03a5cc4ada865fc9","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter04/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter04/","section":"courses","summary":"My example is about weekly shifts, which isn\u0026rsquo;t why I prefer weekly shifts. Weekly shifts are not better or worse. I think that if you are one site team, daily shifts are better. You can work weekends extra and make six people deal with the whole week. For 2-3 sites, weekly shifts are more common. A strategy named \u0026ldquo;follow the sun\u0026rdquo; (FTS 1) is usually defined by omitting the night shift from oncall that can be done in 2 or 3 sites.","tags":null,"title":"Daily vs Weekly","type":"docs"},{"authors":null,"categories":null,"content":"In my example, I try to calculate how many non-working hours people spend, and at least in the EU, you have to pay extra for this 1, and it\u0026rsquo;s good to pay them. Still, many people will like not working on weekends more than money. Sleep deprivation is a big problem, and as a manager, you have to think about this. 2\nI have one colleague that works in a company that provides one day of vacation for a week of on-call instead of money, and he likes it.\nOncall Compensation for Software Engineers - https://blog.pragmaticengineer.com/oncall-compensation/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Durmer and D. Dinges, \u0026ldquo;Neurocognitive Consequences of Sleep Deprivation\u0026rdquo;, in Seminars in Neurology, vol. 25, no. 1, 2005.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"527faddb921d3c1a9c67ef1fb5fd9a97","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter05/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter05/","section":"courses","summary":"In my example, I try to calculate how many non-working hours people spend, and at least in the EU, you have to pay extra for this 1, and it\u0026rsquo;s good to pay them. Still, many people will like not working on weekends more than money. Sleep deprivation is a big problem, and as a manager, you have to think about this. 2\nI have one colleague that works in a company that provides one day of vacation for a week of on-call instead of money, and he likes it.","tags":null,"title":"Working vs non-working hours","type":"docs"},{"authors":null,"categories":null,"content":"The primary and secondary layers are for resolving and managing incidents.\nYou need an escalation process for a few reasons.\ninterchangeability for any reason (the primary having problem accessing the internet, traveling, oversleeping, etc.) nobody responds on primary/secondary, and you need to address this (manager on-call) problems that can\u0026rsquo;t be resolved by primary/secondary (security, the legal, executive decision need it) Testing all escalations is always good, especially with executives who aren\u0026rsquo;t paged often. It is always good if all stakeholders have enough information about current incidents and don\u0026rsquo;t step into the process and ask the Incident commander for information.\nIt always is good to have reasonable times for escalations. You should calculate times from your SLAs and be sure if someone breaks it, you are still good with including time to escalate. You can always calculate the worst time and expected time for the acknowledged incident (MTTA - Mean Time to Ack), and it\u0026rsquo;s good metrics for how healthy are your paging and if it works.\nOn-call Metrics There are essential metrics for incidents, and those are MTTD (Mean Time to Detect), MTTR (Mean Time to Resolve), MTTM (Mean Time to Mitigate), MTBF (Mean Time Between Failures). Resolve and Detect are the most critical metrics. First, I will focus on detection than resolution. The solution for experienced developers isn\u0026rsquo;t the issue. But have good observability, and looking for the right signals takes a lot of work.\nsequenceDiagram Start Incident-\u003e\u003e+Incident Trigger:MTTD Incident Trigger-\u003e\u003e+Incident acknowledgment:MTTA Incident Trigger-\u003e\u003e+Incident mitigation:MTTM Start Incident-\u003e\u003e+Incident resolution:MTTR Delivery Metrics There are other valuable metrics called Four Keys from DORA1 2.\nDeployment Frequency — How often an organization successfully releases to production\nLead Time for Changes — The amount of time it takes a commit to get into production\nChange Failure Rate — The percentage of deployments causing a failure in production\nTime to Restore Service — How long it takes an organization to recover from a failure in production\nThese metrics are about delivery pipelines but support how fast you can recover and deploy fixes, which generally helps on-call.\nDevOps Research and Assessment (DORA)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFour Keys project - https://github.com/dora-team/fourkeys\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"5a572c7dbc816b14f8645af806713dd4","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter06/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter06/","section":"courses","summary":"The primary and secondary layers are for resolving and managing incidents.\nYou need an escalation process for a few reasons.\ninterchangeability for any reason (the primary having problem accessing the internet, traveling, oversleeping, etc.) nobody responds on primary/secondary, and you need to address this (manager on-call) problems that can\u0026rsquo;t be resolved by primary/secondary (security, the legal, executive decision need it) Testing all escalations is always good, especially with executives who aren\u0026rsquo;t paged often.","tags":null,"title":"Escalations \u0026 Metrics","type":"docs"},{"authors":null,"categories":null,"content":"For overview about oncalls I recommend read chapter 14 Oncall 1.\nBut now to my example:\nWe have team with primary and secondary roster We can find what is best for us using 1 - 3 sites We are looking how many people we need and how we calculate costs We don’t try to focus on daily shifts that will make example more complicated We don’t use combinations as primary in daily and secondary weekly that make example more complicated too We\u0026rsquo;re skipping many possible combinations, don’t think that is only good way how to do it, this is more about how you think for your use case Some numbers first, calculate and think what is important to you and if you have enough people to achieve at least this minimum number of people with your conditions. My example calculates with two roasters, and we can choose between 1-3 sites.\nOne site We have 8 people that rotate every 4 weeks. If you can share primary and secondary you have to change this into 2 months and next month just change primary and secondary. I don’t recommend making 2 shifts in rows as many teams do. It’s not healthy. Still all these tables are a minimum number of people that isn’t optimal. You need extra capacity for vacations, sick people and not very near that 25%.\nTwo sites We have 8 people rotated every 4 weeks. Recommendation from SRE book is 6 people per site and I agree. This calculation is bare minimum and I will recommend at least 6 people.\nThree sites We have 9 people rotate every 3 weeks. Still I will go with 6 people per site.\nMany other scenarios you can find here in PagerDuty documentation. I recommend looking into it for inspiration.\nTime Zones The last important thing to multi-sites is time zones. You can’t often choose where your teams are located. If you can, try to make it work for oncall, which needs enough distance for shifts, but fewer gaps for communication. It’s hard to find an optimal way.\nThe 8 hours time zone difference works well from my perspective. I make an example with a few time zones and what three sites and two sites look like.\nI worked with US (PST) and EU teams, and common time is very limited, 2-3 hours for communication, but for two site shifts, it works well. If you need some 3rd place, looking east as Singapore or Thailand is good.\nMany US teams use India, but that doesn’t work with Europe for on-calls. The time difference between Paris and Mumbai is 3.5h, which is too small to help with night shifts.\nT. A. Limoncelli, S. R. Chalup, and C. J. Hogan, The Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2: Addison-Wesley, 2014. - https://learning.oreilly.com/library/view/practice-of-cloud/9780133478549/title.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"7b3f044aad836fe428eba35225765e17","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter07/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter07/","section":"courses","summary":"For overview about oncalls I recommend read chapter 14 Oncall 1.\nBut now to my example:\nWe have team with primary and secondary roster We can find what is best for us using 1 - 3 sites We are looking how many people we need and how we calculate costs We don’t try to focus on daily shifts that will make example more complicated We don’t use combinations as primary in daily and secondary weekly that make example more complicated too We\u0026rsquo;re skipping many possible combinations, don’t think that is only good way how to do it, this is more about how you think for your use case Some numbers first, calculate and think what is important to you and if you have enough people to achieve at least this minimum number of people with your conditions.","tags":null,"title":"Weekly oncall","type":"docs"},{"authors":null,"categories":null,"content":"Reasons for creating an on-call market process:\nIt would be best to have an easy way to change shifts for vacation or when people need a free day. People can change days, hours, or whole shifts as they need it.\nPlease make the change 1:1 so that someone doesn\u0026rsquo;t change shifts for money or skip engineering work. We use a Slack channel dedicated to this, and you need tools where making this change is easy and safe to do.\nWe have automated checking for the next two weeks if people haven\u0026rsquo;t overlapped with primary and secondary. That is useful, too, if you have mixed shifts or you have more rosters.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"11322be5e25f37368876ca6417639b5a","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter08/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter08/","section":"courses","summary":"Reasons for creating an on-call market process:\nIt would be best to have an easy way to change shifts for vacation or when people need a free day. People can change days, hours, or whole shifts as they need it.\nPlease make the change 1:1 so that someone doesn\u0026rsquo;t change shifts for money or skip engineering work. We use a Slack channel dedicated to this, and you need tools where making this change is easy and safe to do.","tags":null,"title":"OnCall Market","type":"docs"},{"authors":null,"categories":null,"content":"I recommend that OnCall owners prepare a few valuable reports for me over the years.\nHow many incidents per week have per team. Using regularly for review every week. How many people have incidents in a month, quarter, or six months? How many people had incidents at night last month, quarter, or six months? How many hours have people on oncall in the last six months There is another set of reports that can provide you postmortem tool, but pager software should give you more insight into how people are doing. Other reports are used for getting outliers. You can find if something doesn\u0026rsquo;t work or goes in the wrong direction. At the start, you can have too many incidents; after years, you can have the opposite problem of having too few incidents. You can have people that only have incidents for part of the year. In that case, you must invest more in gaming days and training.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"aeca60ecd9e710fb82d41ec83c1e64ea","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter09/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter09/","section":"courses","summary":"I recommend that OnCall owners prepare a few valuable reports for me over the years.\nHow many incidents per week have per team. Using regularly for review every week. How many people have incidents in a month, quarter, or six months? How many people had incidents at night last month, quarter, or six months? How many hours have people on oncall in the last six months There is another set of reports that can provide you postmortem tool, but pager software should give you more insight into how people are doing.","tags":null,"title":"Reporting","type":"docs"},{"authors":null,"categories":null,"content":"Writing documentation effectively can be achieved by following a framework such as Diátaxis. Diátaxis is a way of thinking about and doing documentation. We will apply similar approach and split documentation into four parts.\nTutorials How-to Guides Runbooks / Playbooks (Google SRE naming) How to create an effective runbook Glossaries Explanation Tutorials What You Need for On-Call - A tutorial that helps new on-call members onboard into the process, tools, and gain all necessary access for the on-call shift.\nPagerDuty Incident Response How-to Guides Standard Operation Procedures (SOP) for deploying, rotating secrets, adding new regions, scale down and up, adding capacity etc. Runbooks for Incidents - every actionable alert needs a runbook, and we should write and test it regularly to ensure it works and is straightforward to understand.\nYou can start with a checklist as mentioned in the SRE Workbook1.\nAdministering production jobs Understanding debugging info \u0026ldquo;Draining\u0026rdquo; traffic away from a cluster Rolling back a bad software push Blocking or rate-limiting unwanted traffic Bringing up additional serving capacity Using the monitoring systems (for alerting and dashboards) Describing the architecture, various components, and dependencies of the services Before going on-call, the team reviewed precise guidelines about the responsibilities of on-call engineers.\nFor example:\nAt the start of each shift, the on-call engineer reads the handoff from the previous shift. The on-call engineer minimizes user impact first, then makes sure the issues are fully addressed. At the end of the shift, the on-call engineer sends a handoff email to the next engineer on-call. Runbooks / Playbooks (Google SRE naming) As The Site Reliability Workbook1 says, playbooks \u0026ldquo;reduce stress, the mean time to repair (MTTR), and the risk of human error.\u0026rdquo;\nAll alerts should be immediately actionable. There should be an action we expect a human to take immediately after they receive the page that the system is unable to take itself. The signal-to-noise ratio should be high to ensure few false positives; a low signal-to-noise ratio raises the risk for on-call engineers to develop alert fatigue.\nJust like new code, new alerts should be thoroughly and thoughtfully reviewed. Each alert should have a corresponding playbook (runbook) entry.\nExample of runbook template How to create an effective runbook There are five attributes of any good runbook; the five As.2 It must be:\nActionable. It’s nice to know the big picture and architecture of a system, but when you are looking for a runbook, you’re looking to take action based on a particular situation. Accessible. If you can’t find the runbook, it doesn’t matter how well it is written. Accurate. If it doesn’t contain truthful information, it’s worse than nothing at all. Authoritative. It is confusing to have more than one runbook for any given process. Adaptable. Systems evolve, and if you can’t change your runbook, the drift will make it unusable. Glossaries Glossaries3 can be helpful for a few reasons:\nGlossaries help you avoid repetition. When you can refer to a definition with a linked explanation, you save time and words. Glossaries ensure consistency in descriptions. If something is explained in multiple ways, it can become confusing. Glossaries enhance the usability of a runbook for engineers at all experience levels. By including a glossary in your runbook, you provide essential explanations for newer engineers and streamline information for more experienced ones. Explanation RFC RFD ADR paper, wiki, anything that helps why things are how they and how works in detail. Google SRE Workbook\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTransposit ITSM Blog on Good Runbooks\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTransposit SRE Blog on Writing Runbook Documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1713394800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713394800,"objectID":"533b1db5ddc6f433303abcf3479c35cf","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter10a/","publishdate":"2024-04-18T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter10a/","section":"courses","summary":"Writing documentation effectively can be achieved by following a framework such as Diátaxis. Diátaxis is a way of thinking about and doing documentation. We will apply similar approach and split documentation into four parts.\nTutorials How-to Guides Runbooks / Playbooks (Google SRE naming) How to create an effective runbook Glossaries Explanation Tutorials What You Need for On-Call - A tutorial that helps new on-call members onboard into the process, tools, and gain all necessary access for the on-call shift.","tags":null,"title":"On-Call Documentation","type":"docs"},{"authors":null,"categories":null,"content":"We can split tools into multiple categories as pagers, incident response, incident workflows, utility and postmortems. More and more tools are trying to be more universal and cover more of it. Here is always what suits you best and how you can combine it.\nYou can find some overview tools but we will focus more about workflow and what you need.\nIf you get the incident process as process workflow, you see these essential parts that you need to cover with your tooling.\nflowchart TD A[Incident starts] --\u003eB(Incident trigger) B --\u003e C{Routing} B --\u003e D{Category} B --\u003e E{Priority} E --\u003e F{Assignment} C --\u003e F{Assignment} D --\u003e F{Assignment} F --\u003e G(Task creation and management) G --\u003e H(Incident acknowledgment) H --\u003e I(SLA management and escalation) I --\u003e K(Incident resolution) K --\u003e J(Incident closure) Incident trigger\nalarms (your observability tooling finds the problem and triggers the pager tool, API, or email) manual (customer or employee finds the problem and files a ticket to investigate, you can have workflow via email, Slack, etc.) Incident routing, categorization, prioritize and assignment\nmake a category if is a ticket or page. You can have a separate category for security or compliance issues you need to set the correct priority. You calculate by your SLO and error budget you can have low and high priority high priority is about waking up the person and try solving the problem right now low priority is about resolving the problem in the next business hours you need to route the incident to the correct team, and the team should have an oncall person assigned by schedule Task creation and management\nit\u0026rsquo;s good practice to treat every incident as a task in your ticketing system (JIRA, JIRA SD, etc.) Incident acknowledgment\nyou need time to acknowledge that priority. On high priority, you should have 5-15 min. For low priority, you should have 8 hours or the next business day if it is over the weekend SLA management and escalation\nSLA for tickets and pages are management decision, and it\u0026rsquo;s essential to have that times corresponding to your business case you need an escalation process if something doesn\u0026rsquo;t work as expected. Engineers can miss pager or have problems connecting to the internet engineers on-call should have a way how to escalate decisions to executives. If they have solutions that significantly impact customers or the company, it\u0026rsquo;s not on them to make the decision that can cost millions. similar to security and compliance issues, you should always have a legal and security team to help there Incident resolution\nmitigate incident is what engineers have to do it first after they mitigate the problem, on-call will deploy the fix and observe that platform is stable then, they will close incident Incident closure\nyou prepare postmortem document their many ways how to do it. What is essential is to make it easy as possible using templates and make it fast soon as possible (the day of the incident or the next day) postmortem review and planning tickets to improve problems that you observe, don\u0026rsquo;t forget deadlines for tickets and review them Requirements for pagers easy change for people on oncall (oncall market) schedule versioning - easy undo changes if make schedule changes support for calendar export mobile app is useful easy way how whitelisting phone numbers for calling to engineers support for voice call and sms, not all people using smartphones. support for Slack is great, and I will recommend, you can have bot that will answer who have oncall good API for your integrations the SDK for your favorite language is great! reporting is useful as we mention in the previous section. There are links to documentation how scheduling can look like in OpsGenie and PagerDuty:\nhttps://docs.opsgenie.com/docs/on-call-schedules-and-rotations https://support.pagerduty.com/docs/schedules Requirements for incident response and workflows look on integration, covers your tooling (ticketing system, communication tools, status pages, etc) security (at least SOC2, ISO 270001 if you need it) try have your process before and test it if you model it in tooling, this is very hard to upfront and you will be iterate over process during time and you want have flexibility and support from tooling to do it. way how to test it changes properly without breaking current workflows! costs (you will pay for all extra tools and their pricing model can\u0026rsquo;t work for everyone) help with postmortem to collect all evidence from Slack (or similar tool) to later documentation Requirements for postmortems There is essential to know what you want from postmortems as a company. I prefer to look at postmortems as a way to improve internal processes and as learning materials for newcomers.\nmany of tooling are good to have written recoding what happened and how we improve things in the future less tools really help with RCA and find important things about your incidents you can look on postmortems as materials for learning new people and that is hardest to do it ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"f86251f8ff00cd86317361e4ebe9eb79","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter10/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter10/","section":"courses","summary":"We can split tools into multiple categories as pagers, incident response, incident workflows, utility and postmortems. More and more tools are trying to be more universal and cover more of it. Here is always what suits you best and how you can combine it.\nYou can find some overview tools but we will focus more about workflow and what you need.\nIf you get the incident process as process workflow, you see these essential parts that you need to cover with your tooling.","tags":null,"title":"Incident process tooling","type":"docs"},{"authors":null,"categories":null,"content":"For support daily oncall, I recommend a few things. Make visible all that your On Call in one place. I will recommend a dedicated Slack channel for oncall. If you collect all notifications and alerts into the channel is very easy to see that something is happening. It would be best to have a dedicated channel for a major incident for communication and not be overwhelmed with automatic messages. Some tools support this out of the box.\nThe good thing from my experience is the Slack bot that helps with these things:\nyou can ask who has oncall now you can ask who had oncall you can ask who will have oncall you can update the topic of the oncall channel, who have the current oncall shift the bot can make specific mentions to people 5 minutes before the shift starts for handover, which is very useful, and you can see people using that. the bot can check if overlaps primary and secondary and report to the oncall market channel. An example of a tool doing that for PagerDuty - https://github.com/apiaryio/pagerduty-overlap-checker don’t forget to make the channel for the oncall market don’t forget to make the channel for specific roasters as customer support, don’t mix everything into one channel search for slack ops, ops bot, or chat ops on the internet and find many inspirations ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"3619e56a2a0c233a34e1f68f25d5dad2","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter11/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter11/","section":"courses","summary":"For support daily oncall, I recommend a few things. Make visible all that your On Call in one place. I will recommend a dedicated Slack channel for oncall. If you collect all notifications and alerts into the channel is very easy to see that something is happening. It would be best to have a dedicated channel for a major incident for communication and not be overwhelmed with automatic messages. Some tools support this out of the box.","tags":null,"title":"Daily Oncall Routine","type":"docs"},{"authors":null,"categories":null,"content":"Postmortems are necessary for learning and should be held whenever an incident occurs 1. You can determine if you need every incident in some short description or skip it for low-severity ones. You must always be sure that nobody is pointing fingers and the whole process is blameless.\nBest Practice: Avoid Blame and Keep It Constructive 2 Blameless postmortems can be challenging to write, because the postmortem format clearly identifies the actions that led to the incident. Removing blame from a postmortem gives people the confidence to escalate issues without fear. It is also important not to stigmatize frequent production of postmortems by a person or team. An atmosphere of blame risks creating a culture in which incidents and issues are swept under the rug, leading to greater risk for the organization\nAnother important thing is data 3. Your investigation has to focus on hypotheses and analyze data using scientific method. Finding the Root Cause is always challenging.\nWe can continue how to set up the postmortem process 4, and inspire there from the best.\nRoot Cause Analysis Methods Open Incident database Five Ps https://cloudpundit.com/2021/10/28/five-p-factors-for-root-cause-analysis/ The Five Ps (described in IT terms) — well, really six Ps, a problem and five P factors — are as follows:\nThe presenting problemis not only the core impact, but also its broader consequences, which all should be examined and addressed. For instance, “The FizzBots service was down” becomes “Our network was unstable, resulting in FizzBots service failure. Our call center was overwhelmed, our customers are mad at us, and we need to pay out on our SLAs.” The precipitating factorsare the things that triggered the incident. There might not be a single trigger, and the trigger might not be a one-time event (i.e. it could be a rising issue that eventually crossed a threshold, such as exhaustion of a connection pool or running out of server capacity). For example, “A network engineer made a typo in a router configuration.” The perpetuating factorsare the things that resulted in the incident continuing (or becoming worse), once triggered. For instance, “When the network was down, application components queued requests, ran out of memory, crashed, and had to be manually recovered.” The predisposing factorsare the long-standing things that made it more likely that a bad situation would result. For instance, “We do not have automation that checks for bad configurations and prevents their propagation.” or “We are running outdated software on our load-balancers that contains a known bug that results in sometimes sending requests to unresponsive backends.” The protective factorsare things that helped to limit the impact and scope (essentially, your resilience mechanisms). For instance, “We have automation that detected the problem and reverted the configuration change, so the network outage duration was brief.” The present factorsare other factors that were relevant to the outcome (including “where we got lucky”). For instance, “A new version of an application component had just been pushed shortly before the network outage, complicating problem diagnosis,” or “The incident began at noon, when much of the ops team was out having lunch, delaying response.” 5 why’s Five why’s - Wikipedia\nIshikawa diagram - Wikipedia\nUsing a Fishbone (or Ishikawa) Diagram to Perform 5-why Analysis | K Bulsuk: Full Speed Ahead\n5 Why - The Five Why’s is still considered as a best practice by many teams and is a common way to run the root-cause analysis process.The idea here is to ask “why” in succession, going deeper and uncovering more information each time. - https://newsletter.pragmaticengineer.com/p/incident-review-best-practices\nThe framework is very easy to get started, when teams don’t do much digging into incidents. However, as Andrew Hatch at LinkedIn shares in the talk Learning More from Complex systems , there are risks to relying on the Five Whys: *The danger of the Five Whys is how, by following it, we might miss out on other root causes of the incident. (\u0026hellip;)***We’re not broadening our understanding.**We’re just trying to narrow down on one thing, fix it, and hope that this will make the incident not happen again.\nhttp://www.kitchensoap.com/wp-content/uploads/2014/09/Velocity2014-PM-Fac-Handout-Debrief.pdf PDCA Root Cause Analysis, Ishikawa Diagrams and the 5 Why’s The scientific method can be integrated into RCA by using cycles of PDCA . The planning phases consist of describing the problem, collecting data and forming a hypothesis. P: Whether freshly formed or taken from an Ishikawa diagram, the hypothesis should make some form of prediction (or plan), such as “measurement deviation” predicting “parts will be measured out of specification.” D: The next step is do – where the hypothesis is evaluated. This could be as simple as measuring a part or as elaborate as designing a new type of test method. C: The check phase is where the results are evaluated and conclusions are formed. A: Act is where the conclusions are acted upon. A hypothesis may be rejected or modified based on new evidence or the results of the testing, or a plan may be created to confirm a supported hypothesis. 6 Sigma Six Sigma: PDCA on Steroids? Untools Ishikawa Diagram | Untools https://sre.google/sre-book/postmortem-culture/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. G. Boysen, \u0026ldquo;Just Culture: A Foundation for Balanced Accountability and Patient Safety\u0026rdquo;, in The Ochsner Journal, Fall 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.jeli.io/blog/what-kind-of-data-can-you-use-and-should-you-use-for-an-investigation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.jeli.io/howie/welcome\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"7c3ff2e5b7db61e7fb22367c9fd77e7d","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter12/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter12/","section":"courses","summary":"Postmortems are necessary for learning and should be held whenever an incident occurs 1. You can determine if you need every incident in some short description or skip it for low-severity ones. You must always be sure that nobody is pointing fingers and the whole process is blameless.\nBest Practice: Avoid Blame and Keep It Constructive 2 Blameless postmortems can be challenging to write, because the postmortem format clearly identifies the actions that led to the incident.","tags":null,"title":"Postmortems","type":"docs"},{"authors":null,"categories":null,"content":"Design on-call is a tough process that constantly changes conditions. The number of people in teams changes quite often, and you can add more people during the hiring and remove people from the team, and that isn’t easy. We don’t discuss handover time for sites and what is recommended.\nIf you choose Wednesday as an SRE book, I recommend changing to a weekly shift or Sunday or Monday. We use Sunday for reporting purposes, and we have an incident review on Monday, but I don’t see this as a problem. Some recommend making changes on Wednesdays, not overlapping with development planning or other company regular meetings.\nThere are time zones that make you think about different times for handover. We had two sites with California (9 am PST) and Central Europe (6 pm UTC+1) was a good time for us. But some combinations can’t work as well. For example Europe and India, the time zone difference are too small. It’s good for cooperation but not suitable for oncall. Sometimes it is good to don’t tie development teams to oncall teams.\nGood luck with designing your first on-call!\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"cf9e06f6d48521ee2260a556b4897283","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/chapter13/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/chapter13/","section":"courses","summary":"Design on-call is a tough process that constantly changes conditions. The number of people in teams changes quite often, and you can add more people during the hiring and remove people from the team, and that isn’t easy. We don’t discuss handover time for sites and what is recommended.\nIf you choose Wednesday as an SRE book, I recommend changing to a weekly shift or Sunday or Monday. We use Sunday for reporting purposes, and we have an incident review on Monday, but I don’t see this as a problem.","tags":null,"title":"Summary","type":"docs"},{"authors":null,"categories":null,"content":" Category Others func Name URL Notes Company Info Base price Pagers Incident Workflows PagerDuty https://www.pagerduty.com/ Long on market, very good reliability and expanding tools to others categories. Pagerduty Inc, NYSE: PD $25/month/seat Pagers OpsGenie https://www.atlassian.com/software/opsgenie Atlassian Corporation, TEAM (NASDAQ) $23/month/seat Pagers Splunk On-Call https://www.splunk.com/en_us/products/on-call.html former VictorOps N/A Pagers Grafana OnCall https://grafana.com/products/oncall/ Incident Response \u0026amp; Management (IRM) Grafana Labs $20/month/seat Pagers PagerTree https://pagertree.com/ $10/month/seat Pagers Iris https://iris.claims/ OSS LinkedIn Pagers Incident Workflows Rootly https://rootly.com Expanded function to pagers (2023) $15 total, last round 2023 N/A Pagers Incident Workflows incident.io https://incident.io/ Expanded function to pagers (2023) $34, last round 2022 $25/month/seat Incident Response Incident Workflows FireHydrant https://firehydrant.com/ $6000/year Incident Response Jeli Slack bot https://www.jeli.io/slack-app Acquired by PagerDuty 2023 Utility Backstage https://backstage.io/ Developer portal you can integrate all you tools into unfied views Utility Statuspage https://www.atlassian.com/software/statuspage Atlassian Corporation, TEAM (NASDAQ) Utility Status.io https://status.io/ Postmortems Jeli https://www.jeli.io/ Acquired by PagerDuty 2023 Pagers zenduty https://www.zenduty.com/ Pagers Datadog https://www.datadoghq.com/blog/incident-response-with-datadog/ Pagers Utility Betterstack https://betterstack.com/ Message replacing PagerDuty, Statuspage and Pingdom for half price $28.6, last round 2024 $230/month for 6 team members and 200 monitors ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"44a3bb272b2d4d635f572bc74a7720b9","permalink":"https://www.prskavec.net/courses/how-to-make-oncall/tools/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/how-to-make-oncall/tools/","section":"courses","summary":"Category Others func Name URL Notes Company Info Base price Pagers Incident Workflows PagerDuty https://www.pagerduty.com/ Long on market, very good reliability and expanding tools to others categories. Pagerduty Inc, NYSE: PD $25/month/seat Pagers OpsGenie https://www.atlassian.com/software/opsgenie Atlassian Corporation, TEAM (NASDAQ) $23/month/seat Pagers Splunk On-Call https://www.splunk.com/en_us/products/on-call.html former VictorOps N/A Pagers Grafana OnCall https://grafana.com/products/oncall/ Incident Response \u0026amp; Management (IRM) Grafana Labs $20/month/seat Pagers PagerTree https://pagertree.com/ $10/month/seat Pagers Iris https://iris.claims/ OSS LinkedIn Pagers Incident Workflows Rootly https://rootly.","tags":null,"title":"Tools table overview","type":"docs"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"As the adoption of infrastructure as code (IaC) practices continues to grow, Terraform by HashiCorp has emerged as a leading tool for defining and provisioning infrastructure through code. Terraform\u0026rsquo;s declarative configuration language allows developers and operations teams to manage infrastructure with unprecedented precision and efficiency. However, with the growth in complexity and scale of infrastructure projects, users often encounter challenges related to orchestration, state management, and configuration across multiple environments. Enter Terramate, a tool designed to address these very challenges and streamline Terraform projects.\nThe Rise of Terramate Terramate seeks to complement Terraform by providing additional structure and tools for managing multiple Terraform stacks, environments, and configurations more effectively. It is engineered to ease the pain points associated with large-scale Terraform deployments, including dependency management, stack management, and environment differentiation.\nKey Features of Terramate Stack Management: Terramate introduces an efficient way to organize Terraform configurations into stacks, making it easier to manage complex infrastructures. This hierarchical organization allows teams to handle various aspects of their infrastructure in a more modular and understandable manner.\nOrchestration: One of Terramate\u0026rsquo;s strengths is its ability to define and manage dependencies between stacks. This ensures that infrastructure components are deployed in the correct order, addressing one of the common challenges in multi-stack Terraform projects. Define and reuse data in stacks by using variables and metadata. Terramate can be seamlessly integrated into CI/CD pipelines, facilitating the automation of testing and deployment workflows. This capability is crucial for teams aiming to implement DevOps practices and looking to streamline their infrastructure deployment processes. Terramate support Terraform, OpenTofu, Terragrunt, Kubernetes, Pulumi, AWS Cloud Formation, AWS Cloud Development Kit (CDK), Azure Resource Manager (ARM), Biceps, and others.\nCode Generation: Generating code is another advantage of Terramate. By leveraging templates and modules, it reduces code duplication and fosters consistency across your infrastructure, making your codebase more maintainable and scalable.\nGit integration: Helps to detect and manage stacks that contain changes in a branch, commit or pull request.\nIs Terramate the Right Tool for You? While Terramate presents solutions to several Terraform-related challenges, whether it\u0026rsquo;s the best fit for your project depends on various factors. These include the complexity of your infrastructure, the specific challenges you face, and your team\u0026rsquo;s workflow preferences. It\u0026rsquo;s essential to consider Terramate alongside other tools and practices within the Terraform ecosystem, such as Terragrunt, to determine the most suitable approach for your needs.\nMy use case is focus make our infrastructure in Azure Cloud automated. Our infrastructure team give us recommendation to make modules smallest as possible to work with their pipelines. It\u0026rsquo;s nice idea but it\u0026rsquo;s very hard to do it with complex infrastructure that have tens of modules. So I decided to use Terramate for my project. Split modules into stacks and use features for ordering to make graph how I want run them to be sure that order is right.\nI really like generating code. I make templates for backend and providers, that can works for everyone. But I can setup templates specific for project. We generate common variables, injected data from others team etc. That simplify many parts that was duplicated on many places.\nConclusion Terramate emerges as a promising solution for Terraform users grappling with the complexities of orchestrating and managing large-scale infrastructure projects. By offering features such as stack management, environment handling, dependency management, and more, Terramate aims to simplify these challenges, making infrastructure as code even more accessible and manageable. Whether you\u0026rsquo;re a seasoned Terraform user or new to infrastructure as code, exploring Terramate could be a step forward in optimizing your IaC practices.\nI will about more details how we using Terramate after our project will be released to public.\n","date":1710069730,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710069730,"objectID":"5962c6ecdc6b506b4edfc0701c17c94c","permalink":"https://www.prskavec.net/post/exploring-terramate/","publishdate":"2024-03-10T12:22:10+01:00","relpermalink":"/post/exploring-terramate/","section":"post","summary":"Terramate seeks to complement Terraform by providing additional structure and tools for managing multiple Terraform stacks, environments, and configurations more effectively. It is engineered to ease the pain points associated with large-scale Terraform deployments, including dependency management, stack management, and environment differentiation","tags":["terraform"],"title":"Exploring Terramate - A Solution to Terraform Orchestration Challenges","type":"post"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"On OpenTelemetry website, you can read the definition what is OpenTelemetry:\nOpenTelemetry is a collection of APIs, SDKs, and tools. Use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help you analyze your software\u0026rsquo;s performance and behavior.\nIt is not vendor free way to instrument your code. In Go, we don\u0026rsquo;t have in Oct 2023 full support. Traces, metrics, logs, are not in the stable version. You can still start using it, at least for traces and metrics.\nI\u0026rsquo;m working on a project in which I need to make traces from Python to my Go API, and we want all stored in logs and traces. I started with Gin framework, which is my default simple framework for APIs. There are many good alternatives, but I like to use this one.\nIt\u0026rsquo;s great if you Google otel gin you get otelgin library for instrument your code.\nYou find good example where you see how to use it.\nr.Use(otelgin.Middleware(\u0026#34;my-server\u0026#34;)) You add a new line to your code with otelgin middleware, and you are done! Or not?\nContext Propagation If you need to connect traces across services, you need setup propagation, and you find in every example of your tracer.\notel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext{}, propagation.Baggage{})) I will focus on propagation.TraceContext{}. TraceContext is W3C spec on how to pass information about TraceId, SpanId across services in Header traceparent. All work for you if you call another system. But it doesn\u0026rsquo;t work for me.\nI tested with this command:\ncurl -H \u0026#34;traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\u0026#34; http://localhost:8080 I can\u0026rsquo;t find my correct traceId in my logs.\nYou need to configure otelgin middleware with the propagator too. Not just tracer.\notelginOption := otelgin.WithPropagators(propagation.TraceContext{}) r.Use(otelgin.Middleware(\u0026#34;my-server\u0026#34;, otelginOption)) After that, all start working.\nHow to debug it? I use two ways to debug.\nFirst, I\u0026rsquo;m using otel-desktorp-viewer to debug traces locally without running an observability server.\nSecond, I\u0026rsquo;m using log/slog library for logging (available in Go 1.21+). I make simple logging middleware for Gin that records traceId and spanId.\nfunc GinSlogMiddleware(logger *slog.Logger) gin.HandlerFunc { return func(c *gin.Context) { start := time.Now().UTC() path := c.Request.URL.Path query := c.Request.URL.RawQuery c.Next() ctx := c.Request.Context() end := time.Now().UTC() latency := end.Sub(start) fields := slog.Group(\u0026#34;http\u0026#34;, slog.Int(\u0026#34;status\u0026#34;, c.Writer.Status()), slog.String(\u0026#34;method\u0026#34;, c.Request.Method), slog.String(\u0026#34;path\u0026#34;, path), slog.String(\u0026#34;query\u0026#34;, query), slog.String(\u0026#34;ip\u0026#34;, c.ClientIP()), slog.String(\u0026#34;user-agent\u0026#34;, c.Request.UserAgent()), slog.Duration(\u0026#34;latency\u0026#34;, latency), ) if len(c.Errors) \u0026gt; 0 { for _, e := range c.Errors.Errors() { logger.ErrorContext(ctx, e, fields) } } else { logger.InfoContext(ctx, path, fields) } } } You can see that the key is logging with Context, which contains information about traces. Context handler will log information about traces.\ntype ContextHandler struct { slog.Handler } func (h ContextHandler) Handle(ctx context.Context, r slog.Record) error { r.AddAttrs(h.addTraceFromContext(ctx)...) return h.Handler.Handle(ctx, r) } func (h ContextHandler) addTraceFromContext(ctx context.Context) (as []slog.Attr) { if ctx == nil { return } span := trace.SpanContextFromContext(ctx) traceID := span.TraceID().String() spanID := span.SpanID().String() traceGroup := slog.Group(\u0026#34;trace\u0026#34;, slog.String(\u0026#34;id\u0026#34;, traceID)) spanGroup := slog.Group(\u0026#34;span\u0026#34;, slog.String(\u0026#34;id\u0026#34;, spanID)) as = append(as, traceGroup) as = append(as, spanGroup) return } And my logger init looks as this:\nfunc InitLog() *slog.Logger { jsonHandler := slog.NewJSONHandler(os.Stdout) ctxHandler := ContextHandler{jsonHandler} return slog.New(ctxHandler) } You can make a custom JSONHandler if you need to override the logging format, for example, for support Elastic ECS if your destination is Elastic stack.\nResources and libraries supporting log/slog aren\u0026rsquo;t always ready, but I expected that will resolved in the future.\n","date":1697875614,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697875614,"objectID":"94ca1a5bb9d4a524a3c38c23bea4bc5d","permalink":"https://www.prskavec.net/post/otel-gin-go/","publishdate":"2023-10-21T09:06:54+01:00","relpermalink":"/post/otel-gin-go/","section":"post","summary":"How make W3Context work for your application in Go Gin Framework","tags":["otel","go"],"title":"OpenTelemetry and Go Gin Framework","type":"post"},{"authors":[],"categories":null,"content":"OpenTelemetry as best way how to instrument your CICD pipeline. Talk cover observability CICD tools and show demo how instrument Github Action using Honeycomb buildevents and manual instrumentation in project using Dagger.io for pipelines.\nHoneycomb buildevents\nTrace\nEquinix-labs Otel CLI\nDagger.io manual OTEL instrumentation\u0026quot;\nRepository with Dagger.io example\n","date":1695837600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695837600,"objectID":"ef37e8f2f78b22a06b1e5fa17cd0ddbe","permalink":"https://www.prskavec.net/talk/2023-09-27-cloudnative/","publishdate":"2023-09-27T20:00:00Z","relpermalink":"/talk/2023-09-27-cloudnative/","section":"talk","summary":"OpenTelemetry as best way how to instrument your CICD pipeline. Talk cover observability CICD tools and show demo how instrument Github Action using Honeycomb buildevents and manual instrumentation in project using Dagger.io for pipelines.","tags":[],"title":"OpenTelemetry as best way how to instrument your CICD pipeline","type":"talk"},{"authors":[],"categories":null,"content":"Disclaimer: I work for Pure Storage, which provides and sells flash storage for enterprise customers. We have a PDS (Portworx Data Services) service that offers stateful services (PostgreSQL, Kafka, Cassandra, MongoDB, Consul, Mysql, Redis, Elasticsearch, Couchbase, RabbitMQ, ZooKeeper) with a single operator for the deployment of all these services. We have a few other operators for backup and operations. All are based on the Portworx Kubernetes storage solution, and PDS will work in a cloud or on-premise Kubernetes cluster.\n","date":1683828000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683828000,"objectID":"9cfa24352f13aede0d9f6ed45d7e9402","permalink":"https://www.prskavec.net/talk/2023-05-11-cloudnative/","publishdate":"2023-05-11T20:00:00Z","relpermalink":"/talk/2023-05-11-cloudnative/","section":"talk","summary":"Disclaimer: I work for Pure Storage, which provides and sells flash storage for enterprise customers. We have a PDS (Portworx Data Services) service that offers stateful services (PostgreSQL, Kafka, Cassandra, MongoDB, Consul, Mysql, Redis, Elasticsearch, Couchbase, RabbitMQ, ZooKeeper) with a single operator for the deployment of all these services. We have a few other operators for backup and operations. All are based on the Portworx Kubernetes storage solution, and PDS will work in a cloud or on-premise Kubernetes cluster.","tags":[],"title":"How we run stateful services for customers in Kubernetes","type":"talk"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"Many articles have been written about this problem, but I keep seeing this problem coming back over and over again. Most programming languages don\u0026rsquo;t have basic settings for HTTP made to run in production. We\u0026rsquo;ll demonstrate this with the example of Go, but other languages are often similar, sometimes better, sometimes worse.\nIf you take a standard library, and you want to make a request, it will come out something like this.\nfunc main() { url := \u0026#34;http://localhost:3000\u0026#34; var httpClient = \u0026amp;http.Client{} response, _ := httpClient.Get(url) fmt.Println(response) } All the HTTP client settings are given by default transport.\nvar DefaultTransport RoundTripper = \u0026amp;Transport{ Proxy: ProxyFromEnvironment, DialContext: defaultTransportDialContext(\u0026amp;net.Dialer{ Timeout: 30 * time.Second, KeepAlive: 30 * time.Second, }), ForceAttemptHTTP2: true, MaxIdleConns: 100, IdleConnTimeout: 90 * time.Second, TLSHandshakeTimeout: 10 * time.Second, ExpectContinueTimeout: 1 * time.Second, } Your HTTP client will behave according to that basic setup. All the timeouts are defined, and you can\u0026rsquo;t simply hit that client later.\nIf you want to tweak it, if you need to touch the base timeout, you can set it like this:\nfunc main() { url := \u0026#34;http://localhost:3000\u0026#34; var httpClient = \u0026amp;http.Client{ Timeout: time.Second * 10, } response, _ := httpClient.Get(url) fmt.Println(response) } This will override the 30s timeout to 10s which is about the maximum value I would accept for a synchronous POST request, although it\u0026rsquo;s well beyond what I\u0026rsquo;m willing to expect as a user.\nThe way to keep things under control is to have a custom client where we overload the transport and use custom settings.\ntype HTTPClientSettings struct { Connect time.Duration ConnKeepAlive time.Duration ExpectContinue time.Duration IdleConn time.Duration MaxAllIdleConns int MaxHostIdleConns int ResponseHeader time.Duration TLSHandshake time.Duration } func NewHTTPClientWithSettings(httpSettings HTTPClientSettings) (*http.Client, error) { var client http.Client tr := \u0026amp;http.Transport{ ResponseHeaderTimeout: httpSettings.ResponseHeader, Proxy: http.ProxyFromEnvironment, DialContext: (\u0026amp;net.Dialer{ KeepAlive: httpSettings.ConnKeepAlive, DualStack: true, Timeout: httpSettings.Connect, }).DialContext, MaxIdleConns: httpSettings.MaxAllIdleConns, IdleConnTimeout: httpSettings.IdleConn, TLSHandshakeTimeout: httpSettings.TLSHandshake, MaxIdleConnsPerHost: httpSettings.MaxHostIdleConns, ExpectContinueTimeout: httpSettings.ExpectContinue, } // So client makes HTTP/2 requests err := http2.ConfigureTransport(tr) if err != nil { return \u0026amp;client, err } If we have custom settings, then the usage is similar, just directly specifying the individual parameters.\nfunc main() { url := \u0026#34;http://localhost:3000\u0026#34; httpClient, err := NewHTTPClientWithSettings(HTTPClientSettings{ Connect: 5 * time.Second, ExpectContinue: 1 * time.Second, IdleConn: 90 * time.Second, ConnKeepAlive: 30 * time.Second, MaxAllIdleConns: 100, MaxHostIdleConns: 10, ResponseHeader: 5 * time.Second, TLSHandshake: 5 * time.Second, }) if err != nil { fmt.Println(\u0026#34;Got an error creating custom HTTP client:\u0026#34;) fmt.Println(err) return } response, _ := httpClient.Get(url) fmt.Println(response) } This is a good solution, but it still needs to solve everything. Sometimes we would like a different timeout or deadline for a specific case (PUT, POST), and we want to keep the HTTP client the same. This can be achieved with context.WithTimeout or context.WithCancel context. The usage is then seen in the following example.\nfunc main() { url := \u0026#34;http://localhost:3000\u0026#34; req, err := http.NewRequest(http.MethodGet, url, nil) if err != nil { log.Fatal(err) } ctx, cancel := context.WithCancel(context.Background()) _ = time.AfterFunc(1*time.Second, func() { cancel() }) httpClient, err := NewHTTPClientWithSettings(HTTPClientSettings{ Connect: 5 * time.Second, ExpectContinue: 1 * time.Second, IdleConn: 90 * time.Second, ConnKeepAlive: 30 * time.Second, MaxAllIdleConns: 100, MaxHostIdleConns: 10, ResponseHeader: 5 * time.Second, TLSHandshake: 5 * time.Second, }) if err != nil { fmt.Println(\u0026#34;Got an error creating custom HTTP client:\u0026#34;) fmt.Println(err) log.Fatal(err) } response, _ := httpClient.Do(req.WithContext(ctx)) fmt.Println(response) } Still, it can happen that you are not on the client but on the server side, and you can\u0026rsquo;t solve it entirely well. That\u0026rsquo;s why in Go 1.20, they added ResponseController where you get access to these methods:\nFlush() FlushError() error // alternative Flush returning an error Hijack() (net.Conn, *bufio.ReadWriter, error) SetReadDeadline(deadline time.Time) error SetWriteDeadline(deadline time.Time) error Here you can, for example, extend the request deadline if you need it. Find the complete discussion of timeout handling in Handler on GitHub.\nSo remember to set up HTTP clients for production, test performance, measure latency and adjust your settings accordingly. It\u0026rsquo;s good to know your limits and adjust accordingly. Please keep it to the basic settings in production. It can cause annoying errors that are hard to detect because they often look like random errors.\nIf you\u0026rsquo;re interested in this, come and talk about Go and release 1.20 at the next Prague Go Meetup on February 21.\n","date":1674651301,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674651301,"objectID":"feca06760cb4c482e7b9d524970dbe95","permalink":"https://www.prskavec.net/post/watch_out_for_basic_http_client_settings_in_go/","publishdate":"2023-01-25T13:55:01+01:00","relpermalink":"/post/watch_out_for_basic_http_client_settings_in_go/","section":"post","summary":"Most programming languages don't have basic settings for HTTP made to run in production.","tags":["go"],"title":"Watch out for basic HTTP client settings in Go","type":"post"},{"authors":[],"categories":null,"content":"Dagger is a programmable CI/CD engine that runs your pipelines in containers. Develop your CI/CD pipelines as code, in the same programming language as your application. Dagger executes your pipelines entirely as standard OCI containers. This has several benefits:\nInstant local testing Portability: the same pipeline can run on your local machine, a CI runner, a dedicated server, or any container hosting service. Superior caching: every operation is cached by default, and caching works the same everywhere Compatibility with the Docker ecosystem: if it runs in a container, you can add it to your pipeline Cross-language instrumentation: teams can use each other’s tools without learning each other’s language ","date":1669831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669831200,"objectID":"d8619ae42cd3f4e4b0c6618db2d83948","permalink":"https://www.prskavec.net/talk/2022-11-30-dagger-go-sdk/","publishdate":"2022-11-30T20:00:00Z","relpermalink":"/talk/2022-11-30-dagger-go-sdk/","section":"talk","summary":"Talk is about new Dagger Go SDK","tags":[],"title":"CI pipelines should be code! Dagger Go SDK","type":"talk"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"\u0026ldquo;Pipelines as code\u0026rdquo; is not exactly a new concept, I heard about it a few years ago with the launch of Tecton. I wrote a design document at that time to create a new CI that would accept various rules (Jenkins, CircleCI, TravisCI and Github Actions) and you wouldn\u0026rsquo;t have to learn new syntax. Unfortunately, the project never materialized, so I didn\u0026rsquo;t pursue this much longer.\nSolomon Hykes left Docker a few years ago and started Dagger.io, where they started adopting Cue lang and working on an interesting new project. When I got access to early access I wasn\u0026rsquo;t too excited about it, because writing pipelines in Cue is not something I would recommend to developers. After experiencing how hard it was to push Grafana dashboards in Jsonnet, I\u0026rsquo;m cautious.\nA few weeks ago, they launched a technical preview of Dagger Go SDK and when I tried out the basic demo based on the video, I thought it was great. I decided to test on my project ga-badge that generates badge for Github Actions. Here\u0026rsquo;s the relevant PR with the changes, showing how simple the change is. Then here\u0026rsquo;s a log of when it\u0026rsquo;s dropped. I use Mage for dropping as recommended by the Dagger folks for the multi repository. I\u0026rsquo;m sure there are other ways to do it, but it seemed fine to me at first.\nI\u0026rsquo;ll be talking about the Dagger Go SDK at Go Meetup #9 on November 30, 2022, and if you\u0026rsquo;re interested, stop by the discussion.\n","date":1669640934,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669640934,"objectID":"cbae5dc68ad1afdd565bf6e9c2260b74","permalink":"https://www.prskavec.net/post/pipelines_as_code/","publishdate":"2022-11-28T14:08:54+01:00","relpermalink":"/post/pipelines_as_code/","section":"post","summary":"Using Dagger.io Go SDK for Pipelines implementation.","tags":["docker","oci"],"title":"Pipelines as Code","type":"post"},{"authors":[],"categories":null,"content":"What is fuzzing? Fuzzing is the art of automatic bug detection. Fuzzing aims to stress the application and cause unexpected behavior, resource leaks, or crashes. There are many types of fuzzing, and I will focus show us new possibilities that we get in Go 1.18 to use this tool to improve our code.\n","date":1654020000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654020000,"objectID":"318867ed0daa111c9352159445eb671d","permalink":"https://www.prskavec.net/talk/2022-05-31-fuzzing/","publishdate":"2022-05-31T20:00:00Z","relpermalink":"/talk/2022-05-31-fuzzing/","section":"talk","summary":"Talk is about new Fuzzing function in Go 1.18","tags":[],"title":"Getting started with fuzzing","type":"talk"},{"authors":[],"categories":[],"content":"","date":1604396006,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604396006,"objectID":"9b98b4d9686119a0b0d2a33baf20eeff","permalink":"https://www.prskavec.net/post_template/","publishdate":"2020-11-03T10:33:26+01:00","relpermalink":"/post_template/","section":"","summary":"","tags":[],"title":"Template","type":"page"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"First, there is no such thing as a perfect dashboard. You may think that your dashboard is perfect but the next incident can completely break this perception. This is normal, everything is imperfect in this world. We have to work with that. Improving dashboards is an iterative process. You have to make it a part of the culture in the same way as post mortems or other best practices for running services.\nHow to start I always recommend starting with simple things. First, I’d like to recommend reading Grafana best practices articles1. There are lists of things that will help you formulate the overall high-level strategy. Don’t fill all the things at once - start slow. You can always add more metrics as you go.\nIt’s good to start with the first dashboard and basic things, such as absent metrics (to detect if your metrics are even working) and up/down metrics (are all instances of database healthy?).\nGenerally, it’s good not to forget about observability rules. I prefer The Four Golden Signals2. You can also use the USE method3 or RED method4.\nFinally, add important business metrics that are relevant to your service (number of new registrations, outgoing emails, created teams or projects, etc.).\nWhat to do next Maintaining dashboards is not an easy task. Creating hundreds of dashboards is simple but maintaining them is a nightmare. How to prevent this? I recommend thinking about structure and using the drill-down strategy. Use tools, such as Jsonnet, to generate consistent dashboard over teams, environments and services. You will be able to reuse common parts, such as metrics for load balancers or databases used across the company.\nWhat is a drill-down strategy? It means that you have a central dashboard with overview as a single point of contact for your oncall engineers and then there are more detailed dashboards linked from the central dashboard.\nIn this dashboard, they see the most important things. You need work to minimize the noise there and focus only on important signals.\nYou have to keep in mind that nobody is constantly looking at the dashboard (except the guys working in NOC). You have to think about it and make the visualization and error signaling significant to the person at 2am in the morning, after the pager woke them up. If a signal is visible, the operator can click on a graph and to get to another dashboard that gives more insight into the problem. This can be repeated for more levels of depth. Based on my experience, it is only useful to stick to a maximum of 2 to 4 levels of depth.\nDashboards should be using variables and templates. You can let the user switch between different variable values to display the relevant data for a data region, availability domain or a specific application.\nMake templates for reusable components to make your setup DRY and easy to maintain, such as a common header for all your dashboards.\nLinks to detailed dashboards should also use variables and templates, to minimize the amount of code. For instance, you can reuse the same dashboard type for all your applications.\nMake the process work! Create a process to review the dashboards and include it in your regular operations review. You should iterate on improvements of metrics and dashboards, ideally on a weekly basis. The best way to do this review is to make it a part of your incident post-mortem process. You can use your incidents to inspire you to improve your alarms and margins for your dashboards.\nAlarms have always a higher priority than your dashboards and you should focus on good alarms with actionable runbooks for solving issues.\nDashboards have to add more context and overview for your service. This helps with finding correlation with other events5. If you can, integrate the logs and traces into a single tool or add links between them to find what you need.\nTake regular feedback from the team and operators to make it easy to make changes in alarms and maintain the dashboards.\nhttps://grafana.com/docs/grafana/latest/best-practices/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ntraffic, errors, latency, saturation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nutilization, saturation, errors\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nrate, errors, duration\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nfor example deployments are displayed as annotations in Grafana, logs - Loki and traces - Opentracing/Tempo\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1604056154,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604056154,"objectID":"3e727195020f10ac3360a458e4dcb626","permalink":"https://www.prskavec.net/post/how-to-make-perfect-grafana-dashboard/","publishdate":"2020-10-30T12:09:14+01:00","relpermalink":"/post/how-to-make-perfect-grafana-dashboard/","section":"post","summary":"You may think that your dashboard is perfect but the next incident can completely break this perception.","tags":["grafana"],"title":"How to make the perfect Grafana dashboard?","type":"post"},{"authors":["Ladislav Prskavec"],"categories":[],"content":" Thanks to Daria Grudzien for correction and comments.\nContinuous Integration Engines (CIEs) have a long history. The term Continuous Integration (CI) has been used for the first time by Grady Booch in 1991 12. First known CIE is Tinderbox 3 (1997). Personally I started using CruiseControl (launched in 2001) around 2003. After 2005 Java programmers switched from CruseControl to Hudson. Few years later phpUnderControl (2007) became a popular tool, and later on Hudson gave way to Jenkins (released in 2011). These days Jenkins is being replaced by JenkinsX (2017). Many other CIEs have been developed in the meantime: Teamcity (2006), Bamboo (2008), as well as other commercial versions and cloud first CIEs like Travis (2011) , CircleCI (2016) and Github Actions (2018).\nNowadays we have a wide range of CIEs available on every platform, which solve a variety of problems. To help select the best one for the job, in 2019 I wrote a post \u0026ldquo;What you need from your CI\u0026rdquo; which is still valid.\nToday I’d like to introduce a new question— what is the future of CIE and what we can expect as developers and customers of CIE software.\nModularity These days modularity is key. Architectural changes introduced by tools like JenkinsX (2017) and Tekton pipelines (2018) help in designing new CIEs with that feature in mind. Such an approach allows developers to easily change the language describing workflows by replacing modules which translate workflow description to Tekton CRDs (Custom Resource Definition). I like that approach and believe that the teams behind Tekton and JenkinsX are doing great work on increasing flexibility of workflow development.\nFlexibility Flexibility is the second consideration. I don’t think that all CIEs have to be running on Kubernetes. Good alternatives are HashiCorp Nomad,the simplest platform like AWS Lambda and LambdaCI or simply a local computer. Portable software which can be easily run anywhere can be very useful. This still gives space to different CIEs and can be a major differentiator among them.\nLanguage This approach isn’t available among the existing tooling, but it would be nice to have a de facto standard for describing workflows for CIE. Thanks to the modular every CI can support such a standard workflow description, making switching CIEs for projects would become easy. Currently Yaml format is the king - it’s used by Travis, CircleCI, Github Actions among others. But I hope that we will find a better approach like HCL (HashiCorp Configuration Language) which is more of a language than a serialization format (noyaml.com).\nObservability It’s very hard to make any good decisions without sufficient data. Something I see quite often is a lack of good solutions for handling builds, failing tests, flaky tests etc. For applications we have open metrics, Prometheus is commonly supported by many libraries and applications. An open format like this is missing in CIEs and it would be great to fill those gaps.\nScalability Speed, reliability and cost are all relevant from the perspective of scalability. In the current price model you often pay for VMs which is cons ineffective for teams collocated in one timezone. If you have a “follow the sun” model and similar resource usage across workdays — what about weekends? Many large companies have their own CIE, but often it’s run on dedicated hardware which doesn’t scale up with the number of developers or tasks in the queue.\nIt’s possible to keep the same spending level while improving developer experience by scaling the CI up and down depending on current usage. If you discover that your usage pick lasts for 4-5 hours a day—you can scale down the number of instances running CI outside of those hours. But don’t forget that running builds in parallel needs to be fast by default.\nBringing It All Together There are multiple CIEs available in the market today addressing some of the considerations I listed above. Do you know of a Continuous Integration Engine which addresses all those, or do still we have gaps in the market? Is there space for someone to make a significant impact on the market?\nhttps://en.wikipedia.org/wiki/Continuous_integration\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn 1994, Grady Booch used the phrase continuous integration in Object-Oriented Analysis and Design with Applications (2nd edition)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTinderBox by Mozilla - https://oduinn.com/2014/06/04/farewell-to-tinderbox/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1603476154,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603476154,"objectID":"c78464ce7e126196de300740790a57b4","permalink":"https://www.prskavec.net/post/the-future-of-continuous-integration-engines/","publishdate":"2020-10-23T20:02:34+02:00","relpermalink":"/post/the-future-of-continuous-integration-engines/","section":"post","summary":"Continuous Integration Engines (CIEs) have a long history. The term Continuous Integration (CI) has been used for the first time by Grady Booch in 1991.","tags":["cie"],"title":"The Future of Continuous Integration Engines","type":"post"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"I know Grafana for a long time, but in my previous career, we used different tools as Datadog or other older tools. We switched from Datadog to Grafana last year to unite with other teams in one central place.\nI was looking at how others maintain their dashboards and some teams just edit and store JSON outputs from Dashboards. I heard on Monitorama conference about Jsonnet a few years ago. I thought this would be a better approach than editing JSON files directly.\nGrafana has a library called grafonnet-lib that helps you with creating widgets in grafana via jsonnet. You can find prepared components for a dashboard, annotation, graph, singlestat, template, and more. What I find out later was that many options and parts were missing. You need to edit directly or send PRs into the library. I sent a few PRs to fix it. We found more and more missing parameters and I gave up after seeing that many PR were not merged. Today I found that the Grafana team is working on a better solution. In Grafana 7.0 they are generating jsonnet from spec. I hope this will be a good solution to have all parameters available and without the need to fix it by ourselves.\nHow we structure our dashboards This is list of files in our repository:\n├── Makefile ├── README.md ├── build │ ├── dev │ ├── prod │ └── staging ├── dashboards │ ├── actions.jsonnet │ ├── backend.jsonnet │ ├── mongo.jsonnet │ ├── oncall.jsonnet │ ├── redis.jsonnet │ └── traffic.jsonnet ├── environments │ ├── dev.libsonnet │ ├── prod.libsonnet │ └── staging.libsonnet ├── grafonnet (copy of library) └── templates ├── config.libsonnet ├── header.libsonnet ├── mql.libsonnet The makefile has these commands to prepare dashboards and command for deploy.\n$ make Usage: make \u0026lt;target\u0026gt; Targets: help Display this help clean Clean built dashboards build Build dashboards deploy Deploy dashboards auth Get fresh token from grafana We use the same dashboards for every environment and there are configuration changes that are stored in folder environments. We use one grafana instance for all environments but all settings related include name and title of dashboards are specific for the environment. Dashboards have similar names as Team - Environment - Service.\nIn template folder we have config.libsonnet that is the main config file reused in all dashboards. In the folder we have common parts, such as a header with the message: # Don\\'t edit here. Edit in repository, and make PR there!. We extend code as tutorial with libsonnet files.\nThe command to compile jsonnet into build/*.json looks like:\njsonnet --ext-code-file \u0026#34;env=environments/$(patsubst %/,%, $(dir $(patsubst build/%, %, $@))).libsonnet\u0026#34; dashboards/$(basename $(notdir $@)).jsonnet \u0026gt; $@ From the files in the dashboards folder, we create json output that is split to folders by environments. We are following guidance from Grafana Labs at KubeCon: Foolproof Kubernetes Dashboards for Sleep-Deprived On Calls that recommends Hierarchical Dashboards.\nOur dashboards are organized in this order just on two levels. We have the main one called oncall for overview with summary for traffic, databases, etc. and if you click on that in the main dashboard that opens for you a more detailed dashboard with specific information and you can select region, availability zone/domain or application in a template.\nWe are using a jsonnet standard library to generate the list for templates that are big and easy to maintain.\n.addTemplate( template.new( \u0026#39;id\u0026#39;, \u0026#39;$datasource\u0026#39;, \u0026#39;map($app;\u0026#39; + std.manifestJsonEx(dataMap, \u0026#39;\u0026#39;) + \u0026#39;)\u0026#39;, hide = 2, refresh = 1, ) ) and if we get issues with newlines in a map we can easily fix using standard library too and remove newlines:\n.addTemplate( template.new( \u0026#39;id\u0026#39;, \u0026#39;$datasource\u0026#39;, \u0026#39;map($app;\u0026#39; + std.strReplace(std.manifestJsonEx(dataMap, \u0026#39;\u0026#39;),\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) + \u0026#39;)\u0026#39;, hide = 2, refresh = 1, ) ) Making these changes manually in many places would be exhausting.\nSummary The advantage of using jsonnet is more organized code for dashboards. We try to make it dry and if we have some graph type in the dashboard more times, we create a function to generate it with parameters that are different, such as application name. All code is directly in the dashboard jsonnet file. You can move that to separate files if you need it in more places.\nThe disadvantage of this solution is the learning curve, people don\u0026rsquo;t know jsonnet, and there isn\u0026rsquo;t much documentation on how to learn it. But from the year of experience, I see more benefits.\n","date":1602957461,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602957461,"objectID":"831efde22dac00c56b11723bca311138","permalink":"https://www.prskavec.net/post/grafana-jsonnet/","publishdate":"2020-10-17T19:57:41+02:00","relpermalink":"/post/grafana-jsonnet/","section":"post","summary":"I'm using grafana for more than year and Jsonnet helps me make dashboards easy to maintain and this is my experience with it.","tags":["grafana","jsonnet"],"title":"Grafana dashboards and Jsonnet","type":"post"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"Github introduce new feature for repository that have special name as your nickname on Github. If you add README.md there they display content in profile header.\nI see first on twitter where my friend mention this. I like it and make similar version. Later I see this tweet about automate content of readme.\nI\u0026rsquo;m not big friend with python and I need something else there that I make my own version in Golang using Github Actions.\nFirst I created template for readme and use go to proceed template. I read RSS/Atom from my blog to update readme and I used just Golang standard library. Output is just printed to stdout.\npackage main import ( \u0026#34;encoding/xml\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;text/template\u0026#34; ) // Item type for RSS type Item struct { Title string `xml:\u0026#34;title\u0026#34;` Link string `xml:\u0026#34;link\u0026#34;` Desc string `xml:\u0026#34;description\u0026#34;` GUID string `xml:\u0026#34;guid\u0026#34;` PubDate string `xml:\u0026#34;pubDate\u0026#34;` } // Channel type for RSS type Channel struct { Title string `xml:\u0026#34;title\u0026#34;` Link string `xml:\u0026#34;link\u0026#34;` Desc string `xml:\u0026#34;description\u0026#34;` Items []Item `xml:\u0026#34;item\u0026#34;` } // Rss type for RSS as root type Rss struct { Channel Channel `xml:\u0026#34;channel\u0026#34;` } func readFeed(url string) []string { resp, err := http.Get(url) if err != nil { log.Fatal(err) } defer resp.Body.Close() rss := Rss{} decoder := xml.NewDecoder(resp.Body) err = decoder.Decode(\u0026amp;rss) if err != nil { log.Fatal(err) } output := []string{} for _, item := range rss.Channel.Items { output = append(output, fmt.Sprintf(\u0026#34;[%s](%s)\\n\u0026#34;, item.Title, item.Link)) } return output } func main() { content, err := ioutil.ReadFile(\u0026#34;README.md.template\u0026#34;) if err != nil { log.Fatal(err) } text := string(content) blogURL := os.Getenv(\u0026#34;BLOG_URL\u0026#34;) rssItems := readFeed(blogURL) data := struct { Title string Items []string }{ Title: \u0026#34;Last blog posts\u0026#34;, Items: rssItems, } t, err := template.New(\u0026#34;readme\u0026#34;).Parse(text) if err != nil { log.Fatal(err) } err = t.Execute(os.Stdout, data) if err != nil { log.Fatal(err) } } For automating this I created workflow file .github/workflows/build.yml.\nname: Build README on: push: workflow_dispatch: schedule: - cron: \u0026#34;0 4 * * *\u0026#34; jobs: build: runs-on: ubuntu-latest steps: - name: Check out repo uses: actions/checkout@v2 - name: Set up Go uses: actions/setup-go@v2 with: go-version: \u0026#34;^1.14.3\u0026#34; - run: go version - name: Update README env: BLOG_URL: ${{ secrets.BLOG_URL }} run: |- go run main.go \u0026gt; README.md cat README.md - name: Commit and push if README changed run: |- git diff git config --global user.email \u0026#34;readme-bot@github.com\u0026#34; git config --global user.name \u0026#34;README-bot\u0026#34; git diff --quiet || (git add README.md \u0026amp;\u0026amp; git commit -m \u0026#34;Updated README\u0026#34;) git push I run cron daily, you can change as you need it. I run my program go run main.go and stdout is forward into file and commit into repository if diff exists. Github Actions manage access into repository and you don\u0026rsquo;t need add ssh key or something else. The blog URL is defined in repository \u0026lt;repo\u0026gt;/settings/secrets with name BLOG_URL, but can be directly set in this yaml file too as normal environment variable.\n","date":1594449917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594449917,"objectID":"41451051522612777ac29904af6a96db","permalink":"https://www.prskavec.net/post/github-personal-repo/","publishdate":"2020-07-11T08:45:17+02:00","relpermalink":"/post/github-personal-repo/","section":"post","summary":"How to automate changes in README.md in personal repository in Github.","tags":["github"],"title":"Updating README in Github personal repository","type":"post"},{"authors":[],"categories":null,"content":"I went through classical audits, where you fill tens of excel sheets and then consult with internal audit what and how to improve etc. As SRE (Site Reliability Engineer) deals with monitoring and automation, there is also depriving people of manual work. I have seen how the problem is solved on a large scale (cloud providers). I have experience with Oracle Cloud Infrastructure (OCI) and Amazon Web Services (AWS). I’ll show you how to move from Excel to an automatic, near real-time compliance check solution. Proposed solution using Chef Inspec Framework.\n","date":1579788000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579788000,"objectID":"63a52a85af1d5cd5a338e5a053de82a6","permalink":"https://www.prskavec.net/talk/2020-01-23-itsml/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2020-01-23-itsml/","section":"talk","summary":"Talk is about Compliance as Code in general.","tags":[],"title":"Compliance as Code","type":"talk"},{"authors":[],"categories":null,"content":"","date":1572613200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572613200,"objectID":"53932975237e8047a755141ed3dd2904","permalink":"https://www.prskavec.net/talk/2019-11-01-reactiveconf/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2019-11-01-reactiveconf/","section":"talk","summary":"Talk is JAMStack and tools about them in 2019.","tags":[],"title":"JAMStack in 2019","type":"talk"},{"authors":[],"categories":null,"content":"","date":1564945200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564945200,"objectID":"92f7166c8f47bc43db5aba5048c93a52","permalink":"https://www.prskavec.net/talk/2019-08-04-tinygo/","publishdate":"2017-08-04T00:00:00Z","relpermalink":"/talk/2019-08-04-tinygo/","section":"talk","summary":"Talk is about TinyGo project and demo with Micro:Bit controller.","tags":[],"title":"TinyGo","type":"talk"},{"authors":[],"categories":null,"content":"","date":1559827800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559827800,"objectID":"1c89f66af3405c122b75c8f7ead7cf23","permalink":"https://www.prskavec.net/talk/2019-06-06-teststack-conference/","publishdate":"2017-06-06T00:00:00Z","relpermalink":"/talk/2019-06-06-teststack-conference/","section":"talk","summary":"Talk is focused on Compliance as Code and Infrastructure as Code.","tags":[],"title":"Testing Cloud Infrastructure","type":"talk"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"Running golang test on CircleCI is easy but if you need DIND (docker in docker) that is made by setup_remote_docker is all easy and you have something like this:\nversion: 2 jobs: build: docker: - image: circleci/golang:1.13 environment: TEST_RESULTS: /tmp/test-results steps: - checkout - setup_remote_docker - run: mkdir -p $TEST_RESULTS - restore_cache: keys: - v1-pkg-cache - run: go get gotest.tools/gotestsum - run: docker pull appropriate/curl - run: cd test \u0026amp;\u0026amp; go mod download - run: name: Run unit tests command: | make test-unit - run: cp tmp/unit-report.xml /tmp/test-results/unit-report.xml - save_cache: key: v1-pkg-cache paths: - \u0026#34;/go/pkg\u0026#34; - store_artifacts: path: /tmp/test-results destination: raw-test-output - store_test_results: path: /tmp/test-results But the issue is the network that is limit for security reasons and this helper will not work in DIND mode.\nimport http_helper \u0026#34;github.com/gruntwork-io/terratest/modules/http-helper\u0026#34; // It can take a few seconds for the Docker container boot up, so retry a few times maxRetries := 5 timeBetweenRetries := 2 * time.Second url := fmt.Sprintf(\u0026#34;http://localhost:%d\u0026#34;, serverPort) // Setup a TLS configuration to submit with the helper, a blank struct is acceptable tlsConfig := tls.Config{} // website::tag::5::Verify that we get back a 200 OK with the expected text http_helper.HttpGetWithRetry(t, url, \u0026amp;tlsConfig, 200, expectedServerText, maxRetries, timeBetweenRetries) you need to replace http_helper with docker container with curl:\nopts := \u0026amp;docker.RunOptions{ Command: []string{\u0026#34;--retry\u0026#34;, \u0026#34;5\u0026#34;, \u0026#34;--retry-connrefused\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;http://production_nginx:80/hello\u0026#34;}, OtherOptions: []string{\u0026#34;--network\u0026#34;, \u0026#34;testdockercomposelocal_teststack-network\u0026#34;}, } tag = \u0026#34;appropriate/curl\u0026#34; output := docker.Run(t, tag, opts) assert.Equal(t, expectedServerText, output) The important thing is to have the same network with docker-compose that I\u0026rsquo;m using in the application and this container with curl with correct network parameter. Curl have advantage for support an exponential backoff algorithm.\n","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551830400,"objectID":"1fbe02d9999233c9e73235f4be993dac","permalink":"https://www.prskavec.net/post/terratest-circleci/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/post/terratest-circleci/","section":"post","summary":"How using Terratest in CircleCI with docker in docker.","tags":["circleci"],"title":"Running Terratest in CircleCI","type":"post"},{"authors":["Ladislav Prskavec"],"categories":[],"content":"I clean up old posts and restart this with a new look. I can focus on some best practices that I collected over a year that can help multiple people.\nThis blog is powered by JAMStack. I\u0026rsquo;m using static generator Hugo and theme Academic.\nMy workflow using Github and deploy and publish is using Netlify. Netlify makes a preview site for every PR and that helps me review every change in site.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://www.prskavec.net/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"New start in 2020.","tags":["academia"],"title":"New start in 2020","type":"post"}]